{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMmmgcK30YFAq1rJSVTAbN3",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SinghPrach/30-days-with-python/blob/main/GoogleColab_ATS_ResumeScanner.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://oindrilasen.com/2021/05/build-resume-scanner-using-python-nlp/"
      ],
      "metadata": {
        "id": "OLD2Hi0UzeV4"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zBoQPtmQb4xP",
        "outputId": "2e553f22-67a1-4178-e9fd-10b6302400c0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pdfminer in /usr/local/lib/python3.10/dist-packages (20191125)\n",
            "Requirement already satisfied: pycryptodome in /usr/local/lib/python3.10/dist-packages (from pdfminer) (3.20.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install pdfminer"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install docx2txt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ynp9uc7BdgUV",
        "outputId": "f18f22dc-812a-4011-84a3-8979712ea0e0"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: docx2txt in /usr/local/lib/python3.10/dist-packages (0.8)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import io\n",
        "from pdfminer.converter import TextConverter\n",
        "from pdfminer.pdfinterp import PDFPageInterpreter\n",
        "from pdfminer.pdfinterp import PDFResourceManager\n",
        "from pdfminer.pdfpage import PDFPage\n",
        "import docx2txt\n",
        "import re\n",
        "import operator\n",
        "import pandas as pd\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "set(stopwords.words('english'))\n",
        "from wordcloud import WordCloud\n",
        "from nltk.probability import FreqDist\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4bvog9OpcY2O",
        "outputId": "cd2c1850-85c8-4f43-efb2-50558e7acc23"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Read PDF Resume"
      ],
      "metadata": {
        "id": "E2QxrPu2eIMK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def read_pdf_resume(pdf_doc):\n",
        "    resource_manager = PDFResourceManager()\n",
        "    fake_file_handle = io.StringIO()\n",
        "    converter = TextConverter(resource_manager, fake_file_handle)\n",
        "    page_interpreter = PDFPageInterpreter(resource_manager, converter)\n",
        "    with open(pdf_doc, 'rb') as fh:\n",
        "        for page in PDFPage.get_pages(fh,\n",
        "                                      caching=True,\n",
        "                                      check_extractable=True):\n",
        "            page_interpreter.process_page(page)\n",
        "            text = fake_file_handle.getvalue()\n",
        "    converter.close()\n",
        "    fake_file_handle.close()\n",
        "    if text:\n",
        "        return text"
      ],
      "metadata": {
        "id": "CKbuYx01dRbg"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Read Word Resume"
      ],
      "metadata": {
        "id": "dv4TyzYmsjOf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def read_word_resume(word_doc):\n",
        "     resume = docx2txt.process(word_doc)\n",
        "     resume = str(resume)\n",
        "     #print(resume)\n",
        "     text =  ''.join(resume)\n",
        "     text = text.replace(\"\\n\", \"\")\n",
        "     if text:\n",
        "         return text"
      ],
      "metadata": {
        "id": "v0e9wXzSsmAH"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "resume_w = read_word_resume('/content/Resume/PrachiSingh.docx')"
      ],
      "metadata": {
        "id": "kS6j-szLN982"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(resume_w)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pozwgIY4OIBH",
        "outputId": "9f2db56c-5566-4be3-f8cc-218a22473adb"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PRACHI SINGH Bangalore, Karnataka-560038, India | 6204607306 | singh.indiap@gmail.com | Indiahttps://github.com/SinghPrach/SinghPrach/blob/main/README.mdPROFESSIONAL SUMMARY Detail-oriented Big Data Developer with 2 plus years of experience and a strong background in Data Engineering, Data Manipulation, Data Analysis and data driven strategies by presenting actionable data. Skilled in using Python and SQL and experience in developing ETL pipelines and demonstrated ability in participating in discussions with the business stakeholders. WORK EXPERIENCE Data Engineer, PwC India \t\t\t\t                                                                      July 2022 - PresentDesigned and implemented end-to-end data pipelines for processing large datasets of different file formats (csv, parquet,xslx,xlsb,sql tables), resulting in a 75% improvement in data processing efficiency.Developed a planning tool (optimiser) in Pyspark and python for an FMCG, that resulted in an increase in Gross Profit of 10-13 Crores INR pa for the corresponding product segment and STN increase of over 20% wrt the set target.Collaborated with data scientists, business analysts, Subject Matter Experts, clients to understand data requirements and provide actionable insights through data transformation and analysis.Automated the routine data tasks using Azure Data Factory and Databricks, saving 3-4 hours per week for the team.Developed business documentations and maintained data engineering processes and best practices for knowledge transfers and delivered over 9-10 documents (per project).Streamlined the demand forecasted at a 10 days level to week level using Pyspark and Spark SQL, resulting in reduction of processing time from around 30 minutes to 15-20 seconds.Maintained datawarehouse using MS SQL Server and used SQL queries for analysing data as per clientâ€™s requirement and issues.Built a Python solution that provided analysis of consumed turnover of the entire business, of over 350K SKUs, produced in over 1500 sourcing units spread across the 6 continents(global development) for an FMCG. Worked with stakeholders to gather data requirements and translate them into technical specifications for the team.Assisted team in debugging the code, which helped save our precious time.Analyzed historical trends of sales data and consumer behaviour keeping promotional discounts and holidays in consideration. This led Business unit take decision for improving sales performance which maximized their revenues.EDUCATIONBachelor of Technology in Aerospace Engineering (B.Tech/B.E.)                                                                   2018-2022 Indian Institute of Engineering Science and Technology (IIEST) ShibpurSKILLSProgramming Languages: Python, Pyspark, SQL, Pandas, NumpyData Technologies: Spark (distributed computing), Hadoop, HiveMethodology: Agile developmentDatabases: MS SQL ServerVisualization: Matplotlib, Microsoft Office(Excel, PowerPoint), SpreadsheetsCloud Analytics Platform: MS Azure (Data Factory, Data Lake Storage(Gen2), Databricks, Synapse, Delta Lake Tables), GCP(Big Query), AWS (undergone training)Soft Skills: Innovation, Innovative, Articulate, Interpersonal and communication skills, Discipline, Drawing Conclusions, Committed, Team working, Enthusiasm, Written and Verbal Communication, Multicultural environment, Problem-Solving Skills\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def clean_job_decsription(jd):\n",
        "     ''' a function to create a word cloud based on the input text parameter'''\n",
        "     ## Clean the Text\n",
        "     # Lower\n",
        "     clean_jd = jd.lower()\n",
        "     # remove punctuation\n",
        "     clean_jd = re.sub(r'[^\\w\\s]', '', clean_jd)\n",
        "     # remove trailing spaces\n",
        "     clean_jd = clean_jd.strip()\n",
        "     # remove numbers\n",
        "     clean_jd = re.sub('[0-9]+', '', clean_jd)\n",
        "     # tokenize\n",
        "     clean_jd = word_tokenize(clean_jd)\n",
        "     # remove stop words\n",
        "     stop = stopwords.words('english')\n",
        "     clean_jd = [w for w in clean_jd if not w in stop]\n",
        "     return(clean_jd)"
      ],
      "metadata": {
        "id": "HRzxpUvpsnLL"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "job_description_text = \"\"\"Bachelor's degree, OR 3+ years of relevant work experience\n",
        "Preferred Qualifications\n",
        "2 yrs. work experience with a bachelorâ€™s degree or masterâ€™s or advanced degree in an analytical field such as computer science, statistics, finance, economics or relevant area.\n",
        "Working knowledge of Hadoop ecosystem and associated technologies - Hadoop, Hive, Spark\n",
        "Experience in writing Spark and Hive code to process large data sets in Hadoop environments\n",
        "Experience with SQL for extracting, aggregating, and processing big data using Hadoop\n",
        "Development experience in one or more of the following: Scala, Python or Java.\n",
        "Basic understanding of RDBMs viz, MS SQL, DB2, Oracle, etc for data retrieval\n",
        "Experience using VCS like Git\n",
        "Understanding of Kafka, Apache Hudi will be a plus.\"\"\""
      ],
      "metadata": {
        "id": "hAxaLvODNtnB"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "clean_jd = clean_job_decsription(job_description_text)\n",
        "print(clean_jd)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0aIb9v8RNvpf",
        "outputId": "51ccd09e-f0b5-4053-ddc4-20db50fefe33"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['bachelors', 'degree', 'years', 'relevant', 'work', 'experience', 'preferred', 'qualifications', 'yrs', 'work', 'experience', 'bachelors', 'degree', 'masters', 'advanced', 'degree', 'analytical', 'field', 'computer', 'science', 'statistics', 'finance', 'economics', 'relevant', 'area', 'working', 'knowledge', 'hadoop', 'ecosystem', 'associated', 'technologies', 'hadoop', 'hive', 'spark', 'experience', 'writing', 'spark', 'hive', 'code', 'process', 'large', 'data', 'sets', 'hadoop', 'environments', 'experience', 'sql', 'extracting', 'aggregating', 'processing', 'big', 'data', 'using', 'hadoop', 'development', 'experience', 'one', 'following', 'scala', 'python', 'java', 'basic', 'understanding', 'rdbms', 'viz', 'ms', 'sql', 'db', 'oracle', 'etc', 'data', 'retrieval', 'experience', 'using', 'vcs', 'like', 'git', 'understanding', 'kafka', 'apache', 'hudi', 'plus']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def create_word_cloud(jd):\n",
        "     corpus = jd\n",
        "     fdist = FreqDist(corpus)\n",
        "     #print(fdist.most_common(100))\n",
        "     words = ' '.join(corpus)\n",
        "     words = words.split()\n",
        "     data = dict()\n",
        "     for word in (words):\n",
        "      word = word.lower()\n",
        "      data[word] = data.get(word, 0) + 1\n",
        "      dict(sorted(data.items(), key=operator.itemgetter(1),reverse=True))\n",
        "      word_cloud = WordCloud(width = 800, height = 800,background_color ='white',max_words = 500)\n",
        "      word_cloud.generate_from_frequencies(data)\n",
        "      plt.figure(figsize = (10, 8), edgecolor = 'k')\n",
        "      plt.imshow(word_cloud,interpolation = 'bilinear')\n",
        "      plt.axis(\"off\")\n",
        "      plt.tight_layout(pad = 0)\n",
        "      plt.show()"
      ],
      "metadata": {
        "id": "h8X4dLU-tx17"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_resume_score(text):\n",
        "    cv = CountVectorizer(stop_words='english')\n",
        "    count_matrix = cv.fit_transform(text)\n",
        "    #Print the similarity scores\n",
        "    print(\"\\nSimilarity Scores:\")\n",
        "\n",
        "    #get the match percentage\n",
        "    matchPercentage = cosine_similarity(count_matrix)[0][1] * 100\n",
        "    matchPercentage = round(matchPercentage, 2) # round to two decimal\n",
        "\n",
        "    print(\"Your resume matches about \"+ str(matchPercentage)+ \"% of the job description.\")"
      ],
      "metadata": {
        "id": "pRVOiIM-t1JR"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# resume = read_pdf_resume('/content/Resume/PrachiSingh.pdf')\n",
        "\n",
        "job_description = input(\"\\nEnter the Job Description: \")\n",
        "    ## Get a Keywords Cloud\n",
        "clean_jd = clean_job_decsription(job_description)\n",
        "# create_word_cloud(clean_jd)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y0n7Hm7DvwKn",
        "outputId": "ee7f7446-fcd8-4132-d0bf-f4dc0e75dd29"
      },
      "execution_count": 20,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Enter the Job Description: \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text = [resume_w, job_description_text]\n",
        "    ## Get a Match score\n",
        "get_resume_score(text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gNWFiJnPwvr4",
        "outputId": "d634321c-9690-4558-9eb1-bf663b3f5eaa"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Similarity Scores:\n",
            "Your resume matches about 35.21% of the job description.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# For the same job description, same resume got two different scores, when entered in 2 different formats.\n",
        "# 1. .pdf and 2. .docx\n",
        "# In pdf format, the score was 6.82%\n",
        "# In docx format, the score was 35.21%\n",
        "#### Conclusion: While applying, always apply in the .docx format"
      ],
      "metadata": {
        "id": "iUbMf0PmyJ5x"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}